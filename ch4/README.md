## ch4 신경망 학습

### 손실 함수(loss function)
- [x] 신경망 학습에서는 현재의 상태를 `하나의 지표`로 표현
- [x] `그 지표`를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것
- [x] 신경망 학습에서 사용하는 지표는 **손실 함수**
- [x] `일반적으로 평균 제곱 오차`와 `교차 엔트로피 오차`를 사용
- [x] 신경망 학습에서는 최적의 매개변수를 탐색할 때, Loss Function을 최대한 작게 하는 값을 찾음
- [x] 이때 변수의 기울기 값(=미분값)을 계산하고, 이를 토대로 값을 갱신하는 과정을 반복


#### 평균 제곱 오차(Mean Squared Error: MSE)

![image](https://user-images.githubusercontent.com/83503188/161380730-167d27db-a931-4216-8340-871f0cc1caf2.png)

- yk는 신경망의 출력, 사는 정답 레이블, k는 데이터의 차원 수

![image](https://user-images.githubusercontent.com/83503188/161380735-6fd95089-25af-4815-962d-a88d16c8402a.png)

> 	`원-핫 인코딩`
> 
>	한 원소만 1로 하고 그 외는 0으로 나타내는 표기법

#### 교차 엔트로피 오차(Cross Entropy Error:CEE)

![image](https://user-images.githubusercontent.com/83503188/161380738-5b97b9b3-0681-45e7-9e2d-e76c6812c3cb.png)

### 미니배치 학습
- [x] 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾음
- [x] 훈련 데이터가 100개 있으면 그로부터 계산한 100개의 손실 함수 값들의 합을 지표로 삼음


![image](https://user-images.githubusercontent.com/83503188/161380741-9ec4e221-d1c6-4c8d-bab3-bac2ea37ec7c.png)

- [x] 마지막에 N으로 나누어 정규화
- [x] N으로 나눔으로써 `평균 손실 함수`를 구하는 것
- [x] 훈련 데이터로부터 일부만 골라 학습을 수행 -> 일부: **`미니배치`**
- [x] 미니배치 학습: 훈련 데이터에서 지정한 수의 데이터를 무작위로 골라 학습


### 수치 미분
- [x] 경사법에서는 기울기 값을 기준으로 나아갈 방향을 정함


#### 기울기
- [x] 모든 변수의 편미분을 벡터로 정리한 것을 기울기라고 한다.


#### 경사 하강법
- [x] 신경망 역시 최적의 매개변수(가중치와 편향)를 학습 시에 찾아야 한다.
- [x] 여기에서 최적이란 손실 함수가 최솟값이 될 때의 매개변수 값
- [x] 기울기를 잘 이용해서 함수의 최솟값을 찾으려는 것이 경사 하강법 
  > 학습률(learning rate): 갱신하는 양 

